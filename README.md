# Embedding Service Dockeré•œåƒ

è¿™æ˜¯ä¸€ä¸ªåŸºäºSigLIPæ¨¡å‹çš„embeddingå‘é‡ç”ŸæˆæœåŠ¡ï¼Œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„embeddingç”Ÿæˆã€‚æä¾›äº†CPUå’ŒGPUä¸¤ä¸ªç‰ˆæœ¬çš„Dockeré•œåƒã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ–¼ï¸ **å›¾åƒEmbedding**: æ”¯æŒbase64ç¼–ç ã€URLæˆ–æœ¬åœ°è·¯å¾„çš„å›¾åƒ
- ğŸ“ **æ–‡æœ¬Embedding**: æ”¯æŒå•æ¡æˆ–å¤šæ¡æ–‡æœ¬çš„å‘é‡åŒ–
- ğŸš€ **é«˜æ€§èƒ½**: GPUç‰ˆæœ¬æ”¯æŒCUDAåŠ é€Ÿ
- ğŸ’» **CPUæ”¯æŒ**: CPUç‰ˆæœ¬å¯åœ¨æ— GPUç¯å¢ƒä¸‹è¿è¡Œ
- ğŸ” **å¥åº·æ£€æŸ¥**: å†…ç½®å¥åº·æ£€æŸ¥æ¥å£
- ğŸ“¦ **DockeråŒ–**: å¼€ç®±å³ç”¨çš„Dockeré•œåƒ

## APIæ¥å£

### 1. å›¾åƒEmbedding
```bash
POST /embed
Content-Type: application/json

{
  "images": [
    "data:image/jpeg;base64,/9j/4AAQ...",  # base64ç¼–ç 
    "https://example.com/image.jpg",        # URL
    "/path/to/image.jpg"                    # æœ¬åœ°è·¯å¾„
  ]
}
```

å“åº”:
```json
[
  [0.123, 0.456, ...],  # ç¬¬ä¸€å¼ å›¾åƒçš„embeddingå‘é‡
  [0.789, 0.012, ...]   # ç¬¬äºŒå¼ å›¾åƒçš„embeddingå‘é‡
]
```

### 2. æ–‡æœ¬Embedding
```bash
POST /embed_text
Content-Type: application/json

{
  "texts": ["æ–‡æœ¬1", "æ–‡æœ¬2"]  # æˆ–ä½¿ç”¨ "text": "å•ä¸ªæ–‡æœ¬"
}
```

å“åº”:
```json
[
  [0.123, 0.456, ...],  # ç¬¬ä¸€ä¸ªæ–‡æœ¬çš„embeddingå‘é‡
  [0.789, 0.012, ...]   # ç¬¬äºŒä¸ªæ–‡æœ¬çš„embeddingå‘é‡
]
```

### 3. å¥åº·æ£€æŸ¥
```bash
GET /health
```

å“åº”:
```json
{
  "status": "ok",
  "model": "google/siglip2-so400m-patch16-naflex",
  "device": "cuda",
  "cuda_available": true
}
```

## æ„å»ºé•œåƒ

### CPUç‰ˆæœ¬
```bash
cd /data/embedding-service
docker build -f cpu/Dockerfile -t embedding-service:cpu .
```

### GPUç‰ˆæœ¬
```bash
cd /data/embedding-service
docker build -f gpu/Dockerfile -t embedding-service:gpu .
```

## è¿è¡Œå®¹å™¨

### CPUç‰ˆæœ¬
```bash
docker run -d \
  --name embedding-service-cpu \
  -p 8080:8080 \
  -e MODEL_NAME=google/siglip2-so400m-patch16-naflex \
  embedding-service:cpu
```

### GPUç‰ˆæœ¬ï¼ˆéœ€è¦NVIDIA Dockeræ”¯æŒï¼‰
```bash
docker run -d \
  --name embedding-service-gpu \
  --gpus all \
  -p 8081:8080 \
  -e MODEL_NAME=google/siglip2-so400m-patch16-naflex \
  -e CUDA_VISIBLE_DEVICES=0 \
  embedding-service:gpu
```

## ä½¿ç”¨Docker Compose

### å¯åŠ¨CPUç‰ˆæœ¬
```bash
docker-compose up -d embedding-service-cpu
```

### å¯åŠ¨GPUç‰ˆæœ¬
```bash
docker-compose up -d embedding-service-gpu
```

### åŒæ—¶å¯åŠ¨ä¸¤ä¸ªç‰ˆæœ¬
```bash
docker-compose up -d
```

## ç¯å¢ƒå˜é‡

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `MODEL_NAME` | `google/siglip2-so400m-patch16-naflex` | HuggingFaceæ¨¡å‹åç§° |
| `PORT` | `8080` | æœåŠ¡ç›‘å¬ç«¯å£ |
| `HOST` | `0.0.0.0` | æœåŠ¡ç›‘å¬åœ°å€ |
| `CUDA_VISIBLE_DEVICES` | `0` | GPUç‰ˆæœ¬ä½¿ç”¨çš„GPUè®¾å¤‡ID |

## æŒ‚è½½HuggingFaceç¼“å­˜ï¼ˆå¯é€‰ï¼‰

ä¸ºäº†åŠ é€Ÿæ¨¡å‹åŠ è½½ï¼Œå¯ä»¥å°†HuggingFaceç¼“å­˜ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ï¼š

```bash
docker run -d \
  --name embedding-service-gpu \
  --gpus all \
  -p 8081:8080 \
  -v /path/to/huggingface/cache:/app/.cache/huggingface \
  embedding-service:gpu
```

## æµ‹è¯•API

### æµ‹è¯•å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8080/health
```

### æµ‹è¯•æ–‡æœ¬Embedding
```bash
curl -X POST http://localhost:8080/embed_text \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Hello world", "Test embedding"]}'
```

### æµ‹è¯•å›¾åƒEmbeddingï¼ˆä½¿ç”¨base64ï¼‰
```bash
# é¦–å…ˆå°†å›¾åƒè½¬æ¢ä¸ºbase64
IMAGE_B64=$(base64 -w 0 /path/to/image.jpg)

curl -X POST http://localhost:8080/embed \
  -H "Content-Type: application/json" \
  -d "{\"images\": [\"data:image/jpeg;base64,$IMAGE_B64\"]}"
```

## æ€§èƒ½è¯´æ˜

- **CPUç‰ˆæœ¬**: é€‚åˆå°è§„æ¨¡ä½¿ç”¨æˆ–æµ‹è¯•ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢
- **GPUç‰ˆæœ¬**: æ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¿«10-100å€ï¼ˆå–å†³äºGPUå‹å·ï¼‰

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆæ¨èï¼‰

### ä½¿ç”¨Gunicornç”Ÿäº§ç‰ˆæœ¬

**æ„å»ºç”Ÿäº§ç‰ˆæœ¬é•œåƒ**:
```bash
# CPUç‰ˆæœ¬
docker build -f cpu/Dockerfile.prod -t embedding-service:cpu-prod .

# GPUç‰ˆæœ¬
docker build -f gpu/Dockerfile.prod -t embedding-service:gpu-prod .
```

**ä½¿ç”¨Docker Composeéƒ¨ç½²ç”Ÿäº§ç‰ˆæœ¬**:
```bash
docker-compose -f docker-compose.prod.yml up -d
```

**ç”Ÿäº§ç‰ˆæœ¬ä¼˜åŠ¿**:
- âœ… ä½¿ç”¨Gunicorn WSGIæœåŠ¡å™¨ï¼Œæ€§èƒ½æ›´å¥½
- âœ… æ”¯æŒå¤šè¿›ç¨‹/å¤šçº¿ç¨‹ï¼Œå¹¶å‘èƒ½åŠ›æå‡3-5å€
- âœ… æ›´å¥½çš„èµ„æºç®¡ç†å’Œç¨³å®šæ€§
- âœ… æ”¯æŒä¼˜é›…é‡å¯å’Œå¥åº·æ£€æŸ¥

**æ€§èƒ½å¯¹æ¯”**:
- å¼€å‘ç‰ˆæœ¬ï¼ˆFlaskï¼‰: 30-50å¹¶å‘ï¼Œ15-25 RPS
- ç”Ÿäº§ç‰ˆæœ¬ï¼ˆGunicornï¼‰: 100-200å¹¶å‘ï¼Œ50-100 RPSï¼ˆCPUï¼‰/ 200-400 RPSï¼ˆGPUï¼‰

è¯¦ç»†æ€§èƒ½åˆ†æè¯·å‚è€ƒ [CONCURRENCY_ANALYSIS.md](CONCURRENCY_ANALYSIS.md)

## æ€§èƒ½æµ‹è¯•

### ä½¿ç”¨benchmark.pyå·¥å…·

```bash
# å®‰è£…ä¾èµ–
pip install aiohttp requests

# åŸºç¡€æµ‹è¯•
python benchmark.py --url http://localhost:8080 --concurrency 10 --requests 100

# å‹åŠ›æµ‹è¯•
python benchmark.py --url http://localhost:8080 --concurrency 50 --requests 500
```

## æ³¨æ„äº‹é¡¹

1. GPUç‰ˆæœ¬éœ€è¦NVIDIA Dockerè¿è¡Œæ—¶ï¼ˆnvidia-docker2ï¼‰
2. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ä¸€å®šæ—¶é—´
3. æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦1-2GBï¼‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
4. GPUç‰ˆæœ¬å»ºè®®è‡³å°‘4GBæ˜¾å­˜
5. **ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ç”Ÿäº§ç‰ˆæœ¬ï¼ˆGunicornï¼‰**ï¼Œä¸è¦ä½¿ç”¨Flaskå¼€å‘æœåŠ¡å™¨

## æ•…éšœæ’æŸ¥

### æ£€æŸ¥å®¹å™¨æ—¥å¿—
```bash
docker logs embedding-service-gpu
```

### æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
```bash
docker exec embedding-service-gpu python -c "import torch; print(torch.cuda.is_available())"
```

### æ£€æŸ¥æ¨¡å‹åŠ è½½
æŸ¥çœ‹å®¹å™¨æ—¥å¿—ä¸­çš„æ¨¡å‹åŠ è½½ä¿¡æ¯ï¼Œç¡®è®¤æ¨¡å‹æ˜¯å¦æ­£ç¡®ä¸‹è½½å’ŒåŠ è½½ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨çš„æ¨¡å‹éµå¾ªå…¶åŸå§‹è®¸å¯è¯ã€‚è¯·å‚è€ƒHuggingFaceä¸Šçš„æ¨¡å‹é¡µé¢äº†è§£è¯¦æƒ…ã€‚

