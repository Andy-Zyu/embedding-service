# Embedding Service Dockeré•œåƒ

è¿™æ˜¯ä¸€ä¸ªåŸºäºSigLIPæ¨¡å‹çš„embeddingå‘é‡ç”ŸæˆæœåŠ¡ï¼Œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„embeddingç”Ÿæˆã€‚æä¾›äº†CPUå’ŒGPUä¸¤ä¸ªç‰ˆæœ¬çš„Dockeré•œåƒï¼Œå¹¶æ”¯æŒæŒ‰è¯·æ±‚é€‰æ‹©ä¸åŒæ¨¡å‹ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ–¼ï¸ **å›¾åƒEmbedding**: æ”¯æŒbase64ç¼–ç ã€URLæˆ–æœ¬åœ°è·¯å¾„çš„å›¾åƒ
- ğŸ“ **æ–‡æœ¬Embedding**: æ”¯æŒå•æ¡æˆ–å¤šæ¡æ–‡æœ¬çš„å‘é‡åŒ–
- ğŸš€ **é«˜æ€§èƒ½**: GPUç‰ˆæœ¬æ”¯æŒCUDAåŠ é€Ÿ
- ğŸ’» **CPUæ”¯æŒ**: CPUç‰ˆæœ¬å¯åœ¨æ— GPUç¯å¢ƒä¸‹è¿è¡Œ
- ğŸ” **å¥åº·æ£€æŸ¥**: å†…ç½®å¥åº·æ£€æŸ¥æ¥å£
- ğŸ§© **å¤šæ¨¡å‹é€‰æ‹©**: å¯é…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œè¯·æ±‚ä¸­æŒ‡å®š `model` è¿›è¡Œé€‰æ‹©
- ğŸ“¦ **DockeråŒ–**: å¼€ç®±å³ç”¨çš„Dockeré•œåƒ

## APIæ¥å£

### 1. å›¾åƒEmbedding
```bash
POST /embed
Content-Type: application/json

{
  "images": [
    "data:image/jpeg;base64,/9j/4AAQ...",  # base64ç¼–ç 
    "https://example.com/image.jpg",        # URL
    "/path/to/image.jpg"                    # æœ¬åœ°è·¯å¾„
  ]
}
```

å“åº”:
```json
[
  [0.123, 0.456, ...],  # ç¬¬ä¸€å¼ å›¾åƒçš„embeddingå‘é‡
  [0.789, 0.012, ...]   # ç¬¬äºŒå¼ å›¾åƒçš„embeddingå‘é‡
]
```

### 2. æ–‡æœ¬Embedding
```bash
POST /embed_text
Content-Type: application/json

{
  "texts": ["æ–‡æœ¬1", "æ–‡æœ¬2"]  # æˆ–ä½¿ç”¨ "text": "å•ä¸ªæ–‡æœ¬"
}
```

å“åº”:
```json
[
  [0.123, 0.456, ...],  # ç¬¬ä¸€ä¸ªæ–‡æœ¬çš„embeddingå‘é‡
  [0.789, 0.012, ...]   # ç¬¬äºŒä¸ªæ–‡æœ¬çš„embeddingå‘é‡
]
```

### 3. å¥åº·æ£€æŸ¥
```bash
GET /health
```

å“åº”:
```json
{
  "status": "ok",
  "model": "google/siglip2-so400m-patch16-naflex",
  "device": "cuda",
  "cuda_available": true
}
```

## æ„å»ºé•œåƒ

### é…ç½®å›½å†…é•œåƒæºï¼ˆæ¨èï¼‰

ä¸ºäº†åŠ é€Ÿæ„å»ºï¼Œå·²å†…ç½®ä»¥ä¸‹å›½å†…é•œåƒæºé…ç½®ï¼š
- âœ… **pipé•œåƒæº**: æ¸…åå¤§å­¦é•œåƒï¼ˆ`pypi.tuna.tsinghua.edu.cn`ï¼‰
- âœ… **HuggingFaceé•œåƒ**: `hf-mirror.com`

**DockeråŸºç¡€é•œåƒåŠ é€Ÿ**ï¼ˆå¯é€‰ï¼‰ï¼š
å¦‚éœ€åŠ é€ŸDockeråŸºç¡€é•œåƒæ‹‰å–ï¼Œè¯·é…ç½®Dockeré•œåƒåŠ é€Ÿå™¨ï¼Œè¯¦è§ [DOCKER_MIRROR_SETUP.md](DOCKER_MIRROR_SETUP.md)

### CPUç‰ˆæœ¬
```bash
cd /data/embedding-service
docker build -f cpu/Dockerfile -t embedding-service:cpu .
```

### GPUç‰ˆæœ¬
```bash
cd /data/embedding-service
docker build -f gpu/Dockerfile -t embedding-service:gpu .

# å¦‚æœåœ¨Apple Silicon Macä¸Šæ„å»ºï¼ˆä¼šæ˜¾ç¤ºå¹³å°è­¦å‘Šï¼Œä½†å¯ä»¥æ­£å¸¸æ„å»ºï¼‰
# Dockerä¼šè‡ªåŠ¨å¤„ç†å¹³å°è½¬æ¢ï¼Œæ„å»ºçš„é•œåƒå¯ä»¥åœ¨LinuxæœåŠ¡å™¨ä¸Šä½¿ç”¨
```

## è¿è¡Œå®¹å™¨

### CPUç‰ˆæœ¬
```bash
docker run -d \
  --name embedding-service-cpu \
  -p 8080:8080 \
  -e DEFAULT_MODEL_NAME=google/siglip2-so400m-patch16-naflex \
  -e AVAILABLE_MODELS=google/siglip2-so400m-patch16-naflex,infgrad/stella-mrl-large-zh-v3.5-1792d \
  -e SENTENCE_TRANSFORMERS_MODELS=infgrad/stella-mrl-large-zh-v3.5-1792d \
  embedding-service:cpu
```

### GPUç‰ˆæœ¬ï¼ˆéœ€è¦NVIDIA Dockeræ”¯æŒï¼‰
```bash
docker run -d \
  --name embedding-service-gpu \
  --gpus all \
  -p 8081:8080 \
  -e DEFAULT_MODEL_NAME=google/siglip2-so400m-patch16-naflex \
  -e AVAILABLE_MODELS=google/siglip2-so400m-patch16-naflex,infgrad/stella-mrl-large-zh-v3.5-1792d \
  -e SENTENCE_TRANSFORMERS_MODELS=infgrad/stella-mrl-large-zh-v3.5-1792d \
  -e CUDA_VISIBLE_DEVICES=0 \
  embedding-service:gpu
```

## ä½¿ç”¨Docker Compose

### å•å®ä¾‹éƒ¨ç½²ï¼ˆé€‚åˆä¸­å°è§„æ¨¡ï¼‰

```bash
# å¯åŠ¨CPUç‰ˆæœ¬
docker compose up -d embedding-service-cpu

# å¯åŠ¨GPUç‰ˆæœ¬
docker compose up -d embedding-service-gpu

# åŒæ—¶å¯åŠ¨ä¸¤ä¸ªç‰ˆæœ¬
docker compose up -d
```

### å¤šå®ä¾‹æ¨ªå‘æ‰©å±•ï¼ˆé€‚åˆå¤§è§„æ¨¡å¹¶å‘ï¼‰

**æ–¹æ¡ˆ1: ä½¿ç”¨Docker Compose Scaleï¼ˆæ¨èï¼‰**

```bash
# å¯åŠ¨3ä¸ªCPUå®ä¾‹ + 2ä¸ªGPUå®ä¾‹
docker compose -f docker-compose.scale.yml up -d \
  --scale embedding-service-cpu=3 \
  --scale embedding-service-gpu=2

# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
docker compose -f docker-compose.scale.yml ps
```

**æ–¹æ¡ˆ2: ä½¿ç”¨Nginxè´Ÿè½½å‡è¡¡ï¼ˆç»Ÿä¸€å…¥å£ï¼‰**

```bash
# å¯åŠ¨å¤šå®ä¾‹ + Nginxè´Ÿè½½å‡è¡¡å™¨
docker compose -f docker-compose.scale.yml up -d \
  --scale embedding-service-cpu=3 \
  --scale embedding-service-gpu=2

# é€šè¿‡Nginxè®¿é—®ï¼ˆç«¯å£80ï¼‰
curl http://localhost/health
```

## å¹¶å‘æ€§èƒ½ç­–ç•¥

### Dockerå†…éƒ¨å¹¶å‘ï¼ˆå•å®ä¾‹ä¼˜åŒ–ï¼‰

æ¯ä¸ªDockerå®¹å™¨å†…éƒ¨ä½¿ç”¨ **Gunicorn** è¿›è¡Œå¹¶å‘ä¼˜åŒ–ï¼š

- **CPUç‰ˆæœ¬**: 4 workers Ã— 2 threads = **8å¹¶å‘**
- **GPUç‰ˆæœ¬**: 2 workers Ã— 4 threads = **8å¹¶å‘**

**é€‚ç”¨åœºæ™¯**: ä¸­å°è§„æ¨¡å¹¶å‘ï¼ˆ100-200 RPSï¼‰

### Docker Composeæ¨ªå‘æ‰©å±•ï¼ˆå¤§è§„æ¨¡å¹¶å‘ï¼‰

é€šè¿‡å¯åŠ¨å¤šä¸ªDockerå®ä¾‹å®ç°æ¨ªå‘æ‰©å±•ï¼š

- **3ä¸ªCPUå®ä¾‹**: 3 Ã— 8å¹¶å‘ = **24å¹¶å‘**ï¼Œçº¦ **150-300 RPS**
- **5ä¸ªGPUå®ä¾‹**: 5 Ã— 8å¹¶å‘ = **40å¹¶å‘**ï¼Œçº¦ **1000-2000 RPS**

**é€‚ç”¨åœºæ™¯**: å¤§è§„æ¨¡å¹¶å‘ï¼ˆ1000+ RPSï¼‰

### æ¨èé…ç½®

| å¹¶å‘éœ€æ±‚ | CPUå®ä¾‹æ•° | GPUå®ä¾‹æ•° | é¢„æœŸRPS |
|---------|----------|----------|---------|
| å°è§„æ¨¡ï¼ˆ<100ï¼‰ | 1 | 1 | 50-200 |
| ä¸­è§„æ¨¡ï¼ˆ100-500ï¼‰ | 2-3 | 1-2 | 200-1000 |
| å¤§è§„æ¨¡ï¼ˆ500-2000ï¼‰ | 5-10 | 3-5 | 1000-5000 |
| è¶…å¤§è§„æ¨¡ï¼ˆ2000+ï¼‰ | 10+ | 5+ | 5000+ |

**æ³¨æ„**: 
- Dockerå†…éƒ¨å¹¶å‘ï¼ˆGunicornï¼‰æ˜¯**å•å®ä¾‹ä¼˜åŒ–**ï¼Œå—é™äºå•æœºèµ„æº
- Docker Composeæ¨ªå‘æ‰©å±•æ˜¯**å¤šå®ä¾‹æ‰©å±•**ï¼Œå¯ä»¥çªç ´å•æœºé™åˆ¶
- **æ¨è**: å…ˆç”¨Dockerå†…éƒ¨å¹¶å‘ï¼Œéœ€è¦æ›´é«˜å¹¶å‘æ—¶å†æ¨ªå‘æ‰©å±•

## ç¯å¢ƒå˜é‡

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `DEFAULT_MODEL_NAME` | `google/siglip2-so400m-patch16-naflex` | é»˜è®¤æ¨¡å‹åç§° |
| `AVAILABLE_MODELS` | `google/...` | å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ |
| `PRELOAD_MODELS` | `0` | æ˜¯å¦å¯åŠ¨æ—¶é¢„åŠ è½½å…¨éƒ¨æ¨¡å‹ |
| `SENTENCE_TRANSFORMERS_MODELS` | `infgrad/...` | ä½¿ç”¨ SentenceTransformers åŠ è½½çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ |
| `PORT` | `8080` | æœåŠ¡ç›‘å¬ç«¯å£ |
| `HOST` | `0.0.0.0` | æœåŠ¡ç›‘å¬åœ°å€ |
| `WORKERS` | `4` (CPU) / `2` (GPU) | Gunicorn workerè¿›ç¨‹æ•° |
| `THREADS` | `2` | æ¯ä¸ªworkerçš„çº¿ç¨‹æ•° |
| `WORKER_CLASS` | `sync` | Workerç±»å‹ï¼ˆsync/gevent/gthreadï¼‰ |
| `TIMEOUT` | `120` | è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |
| `CUDA_VISIBLE_DEVICES` | `0` | GPUç‰ˆæœ¬ä½¿ç”¨çš„GPUè®¾å¤‡ID |
| `AUTO_DETECT_INPUT_TYPE` | `0` | ä»…å¯¹ `/v1/embeddings` ç”Ÿæ•ˆï¼šå½“è¯·æ±‚æœªæ˜¾å¼æä¾› `input_type` æ—¶ï¼Œè‹¥è¾“å…¥**æ•´ä½“çœ‹èµ·æ¥åƒå›¾ç‰‡**ï¼ˆdata:image/ã€å›¾ç‰‡URLã€å›¾ç‰‡è·¯å¾„ï¼‰ï¼Œè‡ªåŠ¨æŒ‰ `image` å¤„ç† |
| `REJECT_MISMATCH_INPUT_TYPE` | `0` | ä»…å¯¹ `/v1/embeddings` ç”Ÿæ•ˆï¼šå½“ `input_type=text` ä½†è¾“å…¥çœ‹èµ·æ¥åƒå›¾ç‰‡æ—¶ç›´æ¥è¿”å› 400ï¼Œé¿å…æŠŠå›¾ç‰‡URL/base64å½“æ–‡æœ¬å¯¼è‡´â€œç–‘ä¼¼å‘é‡å¡Œç¼©â€ |

## æŒ‚è½½HuggingFaceç¼“å­˜ï¼ˆå¯é€‰ï¼‰

ä¸ºäº†åŠ é€Ÿæ¨¡å‹åŠ è½½ï¼Œå¯ä»¥å°†HuggingFaceç¼“å­˜ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ï¼ˆæ¨èä½¿ç”¨é¡¹ç›®å†… `./hf_cache`ï¼‰ï¼š

```bash
docker run -d \
  --name embedding-service-gpu \
  --gpus all \
  -p 8081:8080 \
  -v ./hf_cache:/app/.cache/huggingface \
  embedding-service:gpu
```

## æµ‹è¯•API

### æµ‹è¯•å¥åº·æ£€æŸ¥
```bash
curl http://localhost:8080/health
```

### æµ‹è¯•æ–‡æœ¬Embedding
```bash
curl -X POST http://localhost:8080/embed_text \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Hello world", "Test embedding"]}'
```

### æµ‹è¯•å›¾åƒEmbeddingï¼ˆä½¿ç”¨base64ï¼‰
```bash
# é¦–å…ˆå°†å›¾åƒè½¬æ¢ä¸ºbase64
IMAGE_B64=$(base64 -w 0 /path/to/image.jpg)

curl -X POST http://localhost:8080/embed \
  -H "Content-Type: application/json" \
  -d "{\"images\": [\"data:image/jpeg;base64,$IMAGE_B64\"]}"
```

## æ€§èƒ½è¯´æ˜

- **CPUç‰ˆæœ¬**: é€‚åˆå°è§„æ¨¡ä½¿ç”¨æˆ–æµ‹è¯•ï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢
- **GPUç‰ˆæœ¬**: æ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¿«10-100å€ï¼ˆå–å†³äºGPUå‹å·ï¼‰

**æ‰€æœ‰ç‰ˆæœ¬é»˜è®¤ä½¿ç”¨Gunicornç”Ÿäº§é…ç½®**ï¼Œå·²ä¼˜åŒ–å¹¶å‘æ€§èƒ½ã€‚

è¯¦ç»†æ€§èƒ½åˆ†æè¯·å‚è€ƒ [CONCURRENCY_ANALYSIS.md](CONCURRENCY_ANALYSIS.md)

## æ€§èƒ½æµ‹è¯•

### ä½¿ç”¨benchmark.pyå·¥å…·

```bash
# å®‰è£…ä¾èµ–
pip install aiohttp requests

# åŸºç¡€æµ‹è¯•
python benchmark.py --url http://localhost:8080 --concurrency 10 --requests 100

# å‹åŠ›æµ‹è¯•
python benchmark.py --url http://localhost:8080 --concurrency 50 --requests 500
```

## æ³¨æ„äº‹é¡¹

1. GPUç‰ˆæœ¬éœ€è¦NVIDIA Dockerè¿è¡Œæ—¶ï¼ˆnvidia-docker2ï¼‰
2. é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œéœ€è¦ä¸€å®šæ—¶é—´
3. æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦1-2GBï¼‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
4. GPUç‰ˆæœ¬å»ºè®®è‡³å°‘4GBæ˜¾å­˜
5. **ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨ç”Ÿäº§ç‰ˆæœ¬ï¼ˆGunicornï¼‰**ï¼Œä¸è¦ä½¿ç”¨Flaskå¼€å‘æœåŠ¡å™¨

## æ•…éšœæ’æŸ¥

### æ£€æŸ¥å®¹å™¨æ—¥å¿—
```bash
docker logs embedding-service-gpu
```

### æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
```bash
docker exec embedding-service-gpu python -c "import torch; print(torch.cuda.is_available())"
```

### æ£€æŸ¥æ¨¡å‹åŠ è½½
æŸ¥çœ‹å®¹å™¨æ—¥å¿—ä¸­çš„æ¨¡å‹åŠ è½½ä¿¡æ¯ï¼Œç¡®è®¤æ¨¡å‹æ˜¯å¦æ­£ç¡®ä¸‹è½½å’ŒåŠ è½½ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨çš„æ¨¡å‹éµå¾ªå…¶åŸå§‹è®¸å¯è¯ã€‚è¯·å‚è€ƒHuggingFaceä¸Šçš„æ¨¡å‹é¡µé¢äº†è§£è¯¦æƒ…ã€‚

